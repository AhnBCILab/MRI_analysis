{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fnEg-Tkx7RLn"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils \n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dataset = pd.read_csv('./output.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "K50DHqyM7n_T",
    "outputId": "7dd87746-73a6-4ea5-ced9-d8920f982c41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                                                Filename  ... CC_Anterior\n",
       "0     ADNI_116_S_0370_MPR-R__GradWarp__N3__Scaled_S3...  ...       819.0\n",
       "1     ADNI_023_S_1190_MPR__GradWarp__B1_Correction__...  ...       891.6\n",
       "2     ADNI_099_S_0551_MPR__GradWarp__B1_Correction__...  ...       753.1\n",
       "3     ADNI_141_S_1137_MPR__GradWarp__B1_Correction__...  ...       804.2\n",
       "4     ADNI_027_S_0116_MPR-R__GradWarp__B1_Correction...  ...       752.6\n",
       "...                                                 ...  ...         ...\n",
       "2245  ADNI_131_S_0497_MPR__GradWarp__B1_Correction__...  ...       949.7\n",
       "2246  ADNI_116_S_1243_MPR__GradWarp__N3__Scaled_S389...  ...       664.0\n",
       "2247  ADNI_029_S_1056_MPR__GradWarp__B1_Correction__...  ...      1045.3\n",
       "2248  ADNI_023_S_1126_MPR__GradWarp__B1_Correction__...  ...       976.1\n",
       "2249  ADNI_002_S_0729_MPR__GradWarp__B1_Correction__...  ...       811.2\n",
       "\n",
       "[2250 rows x 98 columns]>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0W0hX8m8rAp"
   },
   "source": [
    "원래는 2251레코드가 있는데, 수기검사를 안한 인원들은 'None' 이라고 나오기 때문에 그런 인원들은 배제하니까 686만남음 ㄷㄷ.... 수기검사빼고 2251레코드로 volume feature들 넣으면 돌아가는데 그것도 accuracy는 높게 나옴... 그대신 레코드가 많아지면 많아질수록 epoch을 늘려야함.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "nuCznXcp8kVN",
    "outputId": "aaf4d05b-ca3f-4c29-c228-5eb6f19115d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb39b227e48>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "  sns.countplot(x='Group',data=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "colab_type": "code",
    "id": "w4wqBZgZ88ff",
    "outputId": "94cd4075-faab-4bd2-fba5-67b48033bef9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Filename', 'SubjectID', 'Visit', 'Group', 'Age', 'Sex', 'MMSE',\n",
       "       'GDSCALE', 'CDR', 'NPI-Q', 'APOE A1', 'APOE A2', 'Study Identifer',\n",
       "       'Weight', 'Series Identifer', 'Acqusisition Type', 'Weighting',\n",
       "       'Pulse Sequence', 'Slice Thickness', 'TE', 'TR', 'TI', 'Coil',\n",
       "       'Flip Angle', 'Acquistion Plane', 'Matrix X', 'Matrix Y', 'Matrix Z',\n",
       "       'Pixel Spacing X', 'Pixel Spacing Y', 'Field Strength', 'BrainSeg',\n",
       "       'BrainSegNotVent', 'BrainSegNotVentSurf', 'VentricleChoroidVol',\n",
       "       'lhCortex', 'rhCortex', 'Cortex', 'lhCerebralWhiteMatter',\n",
       "       'rhCerebralWhiteMatter', 'CerebralWhiteMatter', 'SubCortGray',\n",
       "       'TotalGray', 'SupraTentorial', 'SupraTentorialNotVent',\n",
       "       'SupraTentorialNotVentVox', 'Mask', 'BrainSegVol-to-eTIV',\n",
       "       'MaskVol-to-eTIV', 'lhSurfaceHoles', 'rhSurfaceHoles', 'SurfaceHoles',\n",
       "       'EstimatedTotalIntraCranialVol', 'Left-Lateral-Ventricle',\n",
       "       'Left-Inf-Lat-Vent', 'Left-Cerebellum-White-Matter',\n",
       "       'Left-Cerebellum-Cortex', 'Left-Thalamus-Proper', 'Left-Caudate',\n",
       "       'Left-Putamen', 'Left-Pallidum', '3rd-Ventricle', '4th-Ventricle',\n",
       "       'Brain-Stem', 'Left-Hippocampus', 'Left-Amygdala', 'CSF',\n",
       "       'Left-Accumbens-area', 'Left-VentralDC', 'Left-vessel',\n",
       "       'Left-choroid-plexus', 'Right-Lateral-Ventricle', 'Right-Inf-Lat-Vent',\n",
       "       'Right-Cerebellum-White-Matter', 'Right-Cerebellum-Cortex',\n",
       "       'Right-Thalamus-Proper', 'Right-Caudate', 'Right-Putamen',\n",
       "       'Right-Pallidum', 'Right-Hippocampus', 'Right-Amygdala',\n",
       "       'Right-Accumbens-area', 'Right-VentralDC', 'Right-vessel',\n",
       "       'Right-choroid-plexus', '5th-Ventricle', 'WM-hypointensities',\n",
       "       'Left-WM-hypointensities', 'Right-WM-hypointensities',\n",
       "       'non-WM-hypointensities', 'Left-non-WM-hypointensities',\n",
       "       'Right-non-WM-hypointensities', 'Optic-Chiasm', 'CC_Posterior',\n",
       "       'CC_Mid_Posterior', 'CC_Central', 'CC_Mid_Anterior', 'CC_Anterior'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5GpdqiyQ9NoX"
   },
   "outputs": [],
   "source": [
    "features = [     'VentricleChoroidVol',\n",
    "        'lhCortex', 'rhCortex', 'Cortex', 'SubCortGray',\n",
    "   'TotalGray','BrainSegVol-to-eTIV','Left-Lateral-Ventricle',\n",
    "   'Left-Inf-Lat-Vent',\n",
    "   'Left-Putamen','3rd-Ventricle','Left-Hippocampus', 'Left-Amygdala', 'CSF',\n",
    "   'Left-Accumbens-area','Left-choroid-plexus','Right-Lateral-Ventricle', 'Right-Inf-Lat-Vent'\n",
    "   ,'Right-Putamen','Right-Hippocampus', 'Right-Amygdala',\n",
    "   'Right-Accumbens-area','WM-hypointensities',\n",
    "   'non-WM-hypointensities','CC_Mid_Anterior']\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        dataset = pd.read_csv('./output.csv')\n",
    "        output = dataset['Group'].astype('category').cat.codes\n",
    "        output = output.astype(int)\n",
    "        self.label = output\n",
    "        data = stats.zscore(np.stack([dataset[col].values for col in features],1))\n",
    "        self.data = torch.tensor(data, dtype=torch.float)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # stuff\n",
    "        return (self.data[index], self.label[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JB7ctDl18lEx"
   },
   "source": [
    "수기 검사 200번 돌림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uJ0PesuvnzsZ"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "\n",
    "    super(Model,self).__init__()\n",
    "    l1 = nn.Linear(len(features), 256)\n",
    "    l2 = nn.Linear(256,128)\n",
    "    l3 = nn.Linear(128,64)\n",
    "    l4 = nn.Linear(64,32)\n",
    "    l5 = nn.Linear(32,3)\n",
    " \n",
    "    \n",
    "\n",
    "    self.hidden = nn.Sequential(\n",
    "        l1,\n",
    "        nn.ReLU(),\n",
    "        l2,\n",
    "        nn.ReLU(),\n",
    "        l3,\n",
    "        nn.ReLU(),\n",
    "        l4,\n",
    "        nn.ReLU(),\n",
    "        l5\n",
    "       \n",
    "            )\n",
    "  \n",
    "\n",
    "  def forward(self,x):\n",
    "    x = self.hidden(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "  model.train()\n",
    "\n",
    "  for index, (data, label) in enumerate(train_loader):\n",
    "    data, label = Variable(data), Variable(label)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "    loss = criterion(output, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if index % 10 ==0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "          epoch, index * len(data), len(train_loader.dataset),\n",
    "          100. * index / len(train_loader), loss.data))\n",
    "  \n",
    "  \n",
    "\n",
    " \n",
    "  #plt.plot(range(num_epochs), loss.data)\n",
    "  #plt.ylabel('Loss')\n",
    "  #plt.xlabel('epoch');  \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mevU1dP7C_GV"
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "\n",
    "\n",
    "  model.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  \n",
    "  for data, label in test_loader:\n",
    "    data, label = Variable(data, volatile=True), Variable(label)\n",
    "    \n",
    "    output = model(data)\n",
    "\n",
    "    test_loss += criterion(output, label).data\n",
    "\n",
    "    pred = output.data.max(1, keepdim=True)[1]\n",
    "    correct += pred.eq(label.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.\n",
    "      format(test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellView": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "7wCvRCYViLI4",
    "outputId": "435d41a9-9591-41e1-a420-262a7b978875"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/2025 (0%)]\tLoss: 1.086262\n",
      "Train Epoch: 1 [160/2025 (8%)]\tLoss: 1.060146\n",
      "Train Epoch: 1 [320/2025 (16%)]\tLoss: 1.097253\n",
      "Train Epoch: 1 [480/2025 (24%)]\tLoss: 0.998602\n",
      "Train Epoch: 1 [640/2025 (31%)]\tLoss: 0.857498\n",
      "Train Epoch: 1 [800/2025 (39%)]\tLoss: 0.787669\n",
      "Train Epoch: 1 [960/2025 (47%)]\tLoss: 0.871592\n",
      "Train Epoch: 1 [1120/2025 (55%)]\tLoss: 0.905429\n",
      "Train Epoch: 1 [1280/2025 (63%)]\tLoss: 0.854618\n",
      "Train Epoch: 1 [1440/2025 (71%)]\tLoss: 0.796229\n",
      "Train Epoch: 1 [1600/2025 (79%)]\tLoss: 0.774906\n",
      "Train Epoch: 1 [1760/2025 (87%)]\tLoss: 0.860925\n",
      "Train Epoch: 1 [1920/2025 (94%)]\tLoss: 1.192459\n",
      "\n",
      "Test set: Average loss: 0.0582, Accuracy: 122/225 (54%)\n",
      "\n",
      "Train Epoch: 2 [0/2025 (0%)]\tLoss: 0.718964\n",
      "Train Epoch: 2 [160/2025 (8%)]\tLoss: 0.895075\n",
      "Train Epoch: 2 [320/2025 (16%)]\tLoss: 0.709680\n",
      "Train Epoch: 2 [480/2025 (24%)]\tLoss: 1.124424\n",
      "Train Epoch: 2 [640/2025 (31%)]\tLoss: 0.671289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [800/2025 (39%)]\tLoss: 0.853205\n",
      "Train Epoch: 2 [960/2025 (47%)]\tLoss: 0.973230\n",
      "Train Epoch: 2 [1120/2025 (55%)]\tLoss: 0.832133\n",
      "Train Epoch: 2 [1280/2025 (63%)]\tLoss: 0.801540\n",
      "Train Epoch: 2 [1440/2025 (71%)]\tLoss: 0.762274\n",
      "Train Epoch: 2 [1600/2025 (79%)]\tLoss: 0.756518\n",
      "Train Epoch: 2 [1760/2025 (87%)]\tLoss: 0.864987\n",
      "Train Epoch: 2 [1920/2025 (94%)]\tLoss: 0.769735\n",
      "\n",
      "Test set: Average loss: 0.0577, Accuracy: 129/225 (57%)\n",
      "\n",
      "Train Epoch: 3 [0/2025 (0%)]\tLoss: 0.770952\n",
      "Train Epoch: 3 [160/2025 (8%)]\tLoss: 0.818146\n",
      "Train Epoch: 3 [320/2025 (16%)]\tLoss: 0.761153\n",
      "Train Epoch: 3 [480/2025 (24%)]\tLoss: 0.672026\n",
      "Train Epoch: 3 [640/2025 (31%)]\tLoss: 0.931185\n",
      "Train Epoch: 3 [800/2025 (39%)]\tLoss: 0.858164\n",
      "Train Epoch: 3 [960/2025 (47%)]\tLoss: 0.854241\n",
      "Train Epoch: 3 [1120/2025 (55%)]\tLoss: 1.063162\n",
      "Train Epoch: 3 [1280/2025 (63%)]\tLoss: 0.823585\n",
      "Train Epoch: 3 [1440/2025 (71%)]\tLoss: 0.829763\n",
      "Train Epoch: 3 [1600/2025 (79%)]\tLoss: 0.662460\n",
      "Train Epoch: 3 [1760/2025 (87%)]\tLoss: 0.983073\n",
      "Train Epoch: 3 [1920/2025 (94%)]\tLoss: 0.724401\n",
      "\n",
      "Test set: Average loss: 0.0547, Accuracy: 134/225 (60%)\n",
      "\n",
      "Train Epoch: 4 [0/2025 (0%)]\tLoss: 0.593480\n",
      "Train Epoch: 4 [160/2025 (8%)]\tLoss: 0.791663\n",
      "Train Epoch: 4 [320/2025 (16%)]\tLoss: 0.712520\n",
      "Train Epoch: 4 [480/2025 (24%)]\tLoss: 0.569148\n",
      "Train Epoch: 4 [640/2025 (31%)]\tLoss: 0.597268\n",
      "Train Epoch: 4 [800/2025 (39%)]\tLoss: 0.887257\n",
      "Train Epoch: 4 [960/2025 (47%)]\tLoss: 0.838920\n",
      "Train Epoch: 4 [1120/2025 (55%)]\tLoss: 0.868994\n",
      "Train Epoch: 4 [1280/2025 (63%)]\tLoss: 0.945167\n",
      "Train Epoch: 4 [1440/2025 (71%)]\tLoss: 0.942669\n",
      "Train Epoch: 4 [1600/2025 (79%)]\tLoss: 0.556657\n",
      "Train Epoch: 4 [1760/2025 (87%)]\tLoss: 0.795371\n",
      "Train Epoch: 4 [1920/2025 (94%)]\tLoss: 0.871181\n",
      "\n",
      "Test set: Average loss: 0.0514, Accuracy: 150/225 (67%)\n",
      "\n",
      "Train Epoch: 5 [0/2025 (0%)]\tLoss: 0.585820\n",
      "Train Epoch: 5 [160/2025 (8%)]\tLoss: 0.702285\n",
      "Train Epoch: 5 [320/2025 (16%)]\tLoss: 0.733988\n",
      "Train Epoch: 5 [480/2025 (24%)]\tLoss: 0.613393\n",
      "Train Epoch: 5 [640/2025 (31%)]\tLoss: 0.761850\n",
      "Train Epoch: 5 [800/2025 (39%)]\tLoss: 0.671846\n",
      "Train Epoch: 5 [960/2025 (47%)]\tLoss: 0.910815\n",
      "Train Epoch: 5 [1120/2025 (55%)]\tLoss: 0.907698\n",
      "Train Epoch: 5 [1280/2025 (63%)]\tLoss: 0.910503\n",
      "Train Epoch: 5 [1440/2025 (71%)]\tLoss: 0.629653\n",
      "Train Epoch: 5 [1600/2025 (79%)]\tLoss: 0.518807\n",
      "Train Epoch: 5 [1760/2025 (87%)]\tLoss: 0.694952\n",
      "Train Epoch: 5 [1920/2025 (94%)]\tLoss: 0.369647\n",
      "\n",
      "Test set: Average loss: 0.0533, Accuracy: 136/225 (60%)\n",
      "\n",
      "Train Epoch: 6 [0/2025 (0%)]\tLoss: 0.632114\n",
      "Train Epoch: 6 [160/2025 (8%)]\tLoss: 0.493680\n",
      "Train Epoch: 6 [320/2025 (16%)]\tLoss: 0.634139\n",
      "Train Epoch: 6 [480/2025 (24%)]\tLoss: 0.583125\n",
      "Train Epoch: 6 [640/2025 (31%)]\tLoss: 0.801873\n",
      "Train Epoch: 6 [800/2025 (39%)]\tLoss: 0.492544\n",
      "Train Epoch: 6 [960/2025 (47%)]\tLoss: 0.653996\n",
      "Train Epoch: 6 [1120/2025 (55%)]\tLoss: 0.726403\n",
      "Train Epoch: 6 [1280/2025 (63%)]\tLoss: 0.930334\n",
      "Train Epoch: 6 [1440/2025 (71%)]\tLoss: 0.294593\n",
      "Train Epoch: 6 [1600/2025 (79%)]\tLoss: 0.538062\n",
      "Train Epoch: 6 [1760/2025 (87%)]\tLoss: 0.430349\n",
      "Train Epoch: 6 [1920/2025 (94%)]\tLoss: 0.583315\n",
      "\n",
      "Test set: Average loss: 0.0461, Accuracy: 155/225 (69%)\n",
      "\n",
      "Train Epoch: 7 [0/2025 (0%)]\tLoss: 0.760776\n",
      "Train Epoch: 7 [160/2025 (8%)]\tLoss: 0.398462\n",
      "Train Epoch: 7 [320/2025 (16%)]\tLoss: 0.668452\n",
      "Train Epoch: 7 [480/2025 (24%)]\tLoss: 0.350657\n",
      "Train Epoch: 7 [640/2025 (31%)]\tLoss: 0.522255\n",
      "Train Epoch: 7 [800/2025 (39%)]\tLoss: 0.580782\n",
      "Train Epoch: 7 [960/2025 (47%)]\tLoss: 0.683884\n",
      "Train Epoch: 7 [1120/2025 (55%)]\tLoss: 0.492064\n",
      "Train Epoch: 7 [1280/2025 (63%)]\tLoss: 0.395737\n",
      "Train Epoch: 7 [1440/2025 (71%)]\tLoss: 0.531768\n",
      "Train Epoch: 7 [1600/2025 (79%)]\tLoss: 0.527178\n",
      "Train Epoch: 7 [1760/2025 (87%)]\tLoss: 0.403627\n",
      "Train Epoch: 7 [1920/2025 (94%)]\tLoss: 0.555436\n",
      "\n",
      "Test set: Average loss: 0.0468, Accuracy: 151/225 (67%)\n",
      "\n",
      "Train Epoch: 8 [0/2025 (0%)]\tLoss: 0.894577\n",
      "Train Epoch: 8 [160/2025 (8%)]\tLoss: 0.353043\n",
      "Train Epoch: 8 [320/2025 (16%)]\tLoss: 0.278220\n",
      "Train Epoch: 8 [480/2025 (24%)]\tLoss: 0.287405\n",
      "Train Epoch: 8 [640/2025 (31%)]\tLoss: 0.231493\n",
      "Train Epoch: 8 [800/2025 (39%)]\tLoss: 0.401130\n",
      "Train Epoch: 8 [960/2025 (47%)]\tLoss: 0.567635\n",
      "Train Epoch: 8 [1120/2025 (55%)]\tLoss: 0.453196\n",
      "Train Epoch: 8 [1280/2025 (63%)]\tLoss: 0.315386\n",
      "Train Epoch: 8 [1440/2025 (71%)]\tLoss: 0.501305\n",
      "Train Epoch: 8 [1600/2025 (79%)]\tLoss: 0.573205\n",
      "Train Epoch: 8 [1760/2025 (87%)]\tLoss: 0.618432\n",
      "Train Epoch: 8 [1920/2025 (94%)]\tLoss: 0.203824\n",
      "\n",
      "Test set: Average loss: 0.0484, Accuracy: 157/225 (70%)\n",
      "\n",
      "Train Epoch: 9 [0/2025 (0%)]\tLoss: 0.372654\n",
      "Train Epoch: 9 [160/2025 (8%)]\tLoss: 0.248941\n",
      "Train Epoch: 9 [320/2025 (16%)]\tLoss: 0.210128\n",
      "Train Epoch: 9 [480/2025 (24%)]\tLoss: 0.463866\n",
      "Train Epoch: 9 [640/2025 (31%)]\tLoss: 0.247125\n",
      "Train Epoch: 9 [800/2025 (39%)]\tLoss: 0.302979\n",
      "Train Epoch: 9 [960/2025 (47%)]\tLoss: 0.322373\n",
      "Train Epoch: 9 [1120/2025 (55%)]\tLoss: 0.330226\n",
      "Train Epoch: 9 [1280/2025 (63%)]\tLoss: 0.478346\n",
      "Train Epoch: 9 [1440/2025 (71%)]\tLoss: 0.313858\n",
      "Train Epoch: 9 [1600/2025 (79%)]\tLoss: 0.357202\n",
      "Train Epoch: 9 [1760/2025 (87%)]\tLoss: 0.601229\n",
      "Train Epoch: 9 [1920/2025 (94%)]\tLoss: 0.411294\n",
      "\n",
      "Test set: Average loss: 0.0460, Accuracy: 157/225 (70%)\n",
      "\n",
      "Train Epoch: 10 [0/2025 (0%)]\tLoss: 0.382226\n",
      "Train Epoch: 10 [160/2025 (8%)]\tLoss: 0.143236\n",
      "Train Epoch: 10 [320/2025 (16%)]\tLoss: 0.141643\n",
      "Train Epoch: 10 [480/2025 (24%)]\tLoss: 0.194108\n",
      "Train Epoch: 10 [640/2025 (31%)]\tLoss: 0.251691\n",
      "Train Epoch: 10 [800/2025 (39%)]\tLoss: 0.234323\n",
      "Train Epoch: 10 [960/2025 (47%)]\tLoss: 0.591490\n",
      "Train Epoch: 10 [1120/2025 (55%)]\tLoss: 0.109459\n",
      "Train Epoch: 10 [1280/2025 (63%)]\tLoss: 0.608930\n",
      "Train Epoch: 10 [1440/2025 (71%)]\tLoss: 0.172043\n",
      "Train Epoch: 10 [1600/2025 (79%)]\tLoss: 0.262338\n",
      "Train Epoch: 10 [1760/2025 (87%)]\tLoss: 0.252531\n",
      "Train Epoch: 10 [1920/2025 (94%)]\tLoss: 0.221589\n",
      "\n",
      "Test set: Average loss: 0.0395, Accuracy: 165/225 (73%)\n",
      "\n",
      "Train Epoch: 11 [0/2025 (0%)]\tLoss: 0.163599\n",
      "Train Epoch: 11 [160/2025 (8%)]\tLoss: 0.379546\n",
      "Train Epoch: 11 [320/2025 (16%)]\tLoss: 0.529635\n",
      "Train Epoch: 11 [480/2025 (24%)]\tLoss: 0.260070\n",
      "Train Epoch: 11 [640/2025 (31%)]\tLoss: 0.460857\n",
      "Train Epoch: 11 [800/2025 (39%)]\tLoss: 0.152450\n",
      "Train Epoch: 11 [960/2025 (47%)]\tLoss: 0.324142\n",
      "Train Epoch: 11 [1120/2025 (55%)]\tLoss: 0.188127\n",
      "Train Epoch: 11 [1280/2025 (63%)]\tLoss: 0.303186\n",
      "Train Epoch: 11 [1440/2025 (71%)]\tLoss: 0.303959\n",
      "Train Epoch: 11 [1600/2025 (79%)]\tLoss: 0.120482\n",
      "Train Epoch: 11 [1760/2025 (87%)]\tLoss: 0.280217\n",
      "Train Epoch: 11 [1920/2025 (94%)]\tLoss: 0.309062\n",
      "\n",
      "Test set: Average loss: 0.0408, Accuracy: 178/225 (79%)\n",
      "\n",
      "Train Epoch: 12 [0/2025 (0%)]\tLoss: 0.373688\n",
      "Train Epoch: 12 [160/2025 (8%)]\tLoss: 0.100774\n",
      "Train Epoch: 12 [320/2025 (16%)]\tLoss: 0.096505\n",
      "Train Epoch: 12 [480/2025 (24%)]\tLoss: 0.068300\n",
      "Train Epoch: 12 [640/2025 (31%)]\tLoss: 0.145427\n",
      "Train Epoch: 12 [800/2025 (39%)]\tLoss: 0.164810\n",
      "Train Epoch: 12 [960/2025 (47%)]\tLoss: 0.367977\n",
      "Train Epoch: 12 [1120/2025 (55%)]\tLoss: 0.316883\n",
      "Train Epoch: 12 [1280/2025 (63%)]\tLoss: 0.781345\n",
      "Train Epoch: 12 [1440/2025 (71%)]\tLoss: 0.255014\n",
      "Train Epoch: 12 [1600/2025 (79%)]\tLoss: 0.224526\n",
      "Train Epoch: 12 [1760/2025 (87%)]\tLoss: 0.445388\n",
      "Train Epoch: 12 [1920/2025 (94%)]\tLoss: 0.461641\n",
      "\n",
      "Test set: Average loss: 0.0417, Accuracy: 183/225 (81%)\n",
      "\n",
      "Train Epoch: 13 [0/2025 (0%)]\tLoss: 0.100663\n",
      "Train Epoch: 13 [160/2025 (8%)]\tLoss: 0.065377\n",
      "Train Epoch: 13 [320/2025 (16%)]\tLoss: 0.105524\n",
      "Train Epoch: 13 [480/2025 (24%)]\tLoss: 0.077691\n",
      "Train Epoch: 13 [640/2025 (31%)]\tLoss: 0.104429\n",
      "Train Epoch: 13 [800/2025 (39%)]\tLoss: 0.082127\n",
      "Train Epoch: 13 [960/2025 (47%)]\tLoss: 0.235517\n",
      "Train Epoch: 13 [1120/2025 (55%)]\tLoss: 0.071761\n",
      "Train Epoch: 13 [1280/2025 (63%)]\tLoss: 0.081558\n",
      "Train Epoch: 13 [1440/2025 (71%)]\tLoss: 0.110593\n",
      "Train Epoch: 13 [1600/2025 (79%)]\tLoss: 0.186985\n",
      "Train Epoch: 13 [1760/2025 (87%)]\tLoss: 0.722648\n",
      "Train Epoch: 13 [1920/2025 (94%)]\tLoss: 0.149476\n",
      "\n",
      "Test set: Average loss: 0.0426, Accuracy: 182/225 (81%)\n",
      "\n",
      "Train Epoch: 14 [0/2025 (0%)]\tLoss: 0.133096\n",
      "Train Epoch: 14 [160/2025 (8%)]\tLoss: 0.141387\n",
      "Train Epoch: 14 [320/2025 (16%)]\tLoss: 0.147185\n",
      "Train Epoch: 14 [480/2025 (24%)]\tLoss: 0.057853\n",
      "Train Epoch: 14 [640/2025 (31%)]\tLoss: 0.094129\n",
      "Train Epoch: 14 [800/2025 (39%)]\tLoss: 0.089213\n",
      "Train Epoch: 14 [960/2025 (47%)]\tLoss: 0.075044\n",
      "Train Epoch: 14 [1120/2025 (55%)]\tLoss: 0.184726\n",
      "Train Epoch: 14 [1280/2025 (63%)]\tLoss: 0.131724\n",
      "Train Epoch: 14 [1440/2025 (71%)]\tLoss: 0.128639\n",
      "Train Epoch: 14 [1600/2025 (79%)]\tLoss: 0.086940\n",
      "Train Epoch: 14 [1760/2025 (87%)]\tLoss: 0.407641\n",
      "Train Epoch: 14 [1920/2025 (94%)]\tLoss: 0.261681\n",
      "\n",
      "Test set: Average loss: 0.0396, Accuracy: 180/225 (80%)\n",
      "\n",
      "Train Epoch: 15 [0/2025 (0%)]\tLoss: 0.050733\n",
      "Train Epoch: 15 [160/2025 (8%)]\tLoss: 0.079596\n",
      "Train Epoch: 15 [320/2025 (16%)]\tLoss: 0.095479\n",
      "Train Epoch: 15 [480/2025 (24%)]\tLoss: 0.133351\n",
      "Train Epoch: 15 [640/2025 (31%)]\tLoss: 0.026325\n",
      "Train Epoch: 15 [800/2025 (39%)]\tLoss: 0.208261\n",
      "Train Epoch: 15 [960/2025 (47%)]\tLoss: 0.211762\n",
      "Train Epoch: 15 [1120/2025 (55%)]\tLoss: 0.097039\n",
      "Train Epoch: 15 [1280/2025 (63%)]\tLoss: 0.256481\n",
      "Train Epoch: 15 [1440/2025 (71%)]\tLoss: 0.046034\n",
      "Train Epoch: 15 [1600/2025 (79%)]\tLoss: 0.021076\n",
      "Train Epoch: 15 [1760/2025 (87%)]\tLoss: 0.132387\n",
      "Train Epoch: 15 [1920/2025 (94%)]\tLoss: 0.066907\n",
      "\n",
      "Test set: Average loss: 0.0656, Accuracy: 181/225 (80%)\n",
      "\n",
      "Train Epoch: 16 [0/2025 (0%)]\tLoss: 0.007463\n",
      "Train Epoch: 16 [160/2025 (8%)]\tLoss: 0.134236\n",
      "Train Epoch: 16 [320/2025 (16%)]\tLoss: 0.018980\n",
      "Train Epoch: 16 [480/2025 (24%)]\tLoss: 0.071087\n",
      "Train Epoch: 16 [640/2025 (31%)]\tLoss: 0.132361\n",
      "Train Epoch: 16 [800/2025 (39%)]\tLoss: 0.128245\n",
      "Train Epoch: 16 [960/2025 (47%)]\tLoss: 0.102272\n",
      "Train Epoch: 16 [1120/2025 (55%)]\tLoss: 0.048992\n",
      "Train Epoch: 16 [1280/2025 (63%)]\tLoss: 0.067231\n",
      "Train Epoch: 16 [1440/2025 (71%)]\tLoss: 0.074944\n",
      "Train Epoch: 16 [1600/2025 (79%)]\tLoss: 0.155426\n",
      "Train Epoch: 16 [1760/2025 (87%)]\tLoss: 0.544000\n",
      "Train Epoch: 16 [1920/2025 (94%)]\tLoss: 0.141062\n",
      "\n",
      "Test set: Average loss: 0.0454, Accuracy: 186/225 (83%)\n",
      "\n",
      "Train Epoch: 17 [0/2025 (0%)]\tLoss: 0.017945\n",
      "Train Epoch: 17 [160/2025 (8%)]\tLoss: 0.054849\n",
      "Train Epoch: 17 [320/2025 (16%)]\tLoss: 0.180102\n",
      "Train Epoch: 17 [480/2025 (24%)]\tLoss: 0.380345\n",
      "Train Epoch: 17 [640/2025 (31%)]\tLoss: 0.079487\n",
      "Train Epoch: 17 [800/2025 (39%)]\tLoss: 0.048634\n",
      "Train Epoch: 17 [960/2025 (47%)]\tLoss: 0.013969\n",
      "Train Epoch: 17 [1120/2025 (55%)]\tLoss: 0.173783\n",
      "Train Epoch: 17 [1280/2025 (63%)]\tLoss: 0.045806\n",
      "Train Epoch: 17 [1440/2025 (71%)]\tLoss: 0.006265\n",
      "Train Epoch: 17 [1600/2025 (79%)]\tLoss: 0.227419\n",
      "Train Epoch: 17 [1760/2025 (87%)]\tLoss: 0.028868\n",
      "Train Epoch: 17 [1920/2025 (94%)]\tLoss: 0.014808\n",
      "\n",
      "Test set: Average loss: 0.0598, Accuracy: 191/225 (85%)\n",
      "\n",
      "Train Epoch: 18 [0/2025 (0%)]\tLoss: 0.034995\n",
      "Train Epoch: 18 [160/2025 (8%)]\tLoss: 0.031590\n",
      "Train Epoch: 18 [320/2025 (16%)]\tLoss: 0.019255\n",
      "Train Epoch: 18 [480/2025 (24%)]\tLoss: 0.011579\n",
      "Train Epoch: 18 [640/2025 (31%)]\tLoss: 0.392494\n",
      "Train Epoch: 18 [800/2025 (39%)]\tLoss: 0.023617\n",
      "Train Epoch: 18 [960/2025 (47%)]\tLoss: 0.130234\n",
      "Train Epoch: 18 [1120/2025 (55%)]\tLoss: 0.125800\n",
      "Train Epoch: 18 [1280/2025 (63%)]\tLoss: 0.036493\n",
      "Train Epoch: 18 [1440/2025 (71%)]\tLoss: 0.053995\n",
      "Train Epoch: 18 [1600/2025 (79%)]\tLoss: 0.095759\n",
      "Train Epoch: 18 [1760/2025 (87%)]\tLoss: 0.064568\n",
      "Train Epoch: 18 [1920/2025 (94%)]\tLoss: 0.174814\n",
      "\n",
      "Test set: Average loss: 0.0452, Accuracy: 188/225 (84%)\n",
      "\n",
      "Train Epoch: 19 [0/2025 (0%)]\tLoss: 0.075330\n",
      "Train Epoch: 19 [160/2025 (8%)]\tLoss: 0.088114\n",
      "Train Epoch: 19 [320/2025 (16%)]\tLoss: 0.009835\n",
      "Train Epoch: 19 [480/2025 (24%)]\tLoss: 0.067245\n",
      "Train Epoch: 19 [640/2025 (31%)]\tLoss: 0.076565\n",
      "Train Epoch: 19 [800/2025 (39%)]\tLoss: 0.050091\n",
      "Train Epoch: 19 [960/2025 (47%)]\tLoss: 0.029468\n",
      "Train Epoch: 19 [1120/2025 (55%)]\tLoss: 0.049212\n",
      "Train Epoch: 19 [1280/2025 (63%)]\tLoss: 0.029718\n",
      "Train Epoch: 19 [1440/2025 (71%)]\tLoss: 0.068629\n",
      "Train Epoch: 19 [1600/2025 (79%)]\tLoss: 0.031378\n",
      "Train Epoch: 19 [1760/2025 (87%)]\tLoss: 0.017959\n",
      "Train Epoch: 19 [1920/2025 (94%)]\tLoss: 0.211189\n",
      "\n",
      "Test set: Average loss: 0.0659, Accuracy: 184/225 (82%)\n",
      "\n",
      "Train Epoch: 20 [0/2025 (0%)]\tLoss: 0.034709\n",
      "Train Epoch: 20 [160/2025 (8%)]\tLoss: 0.029871\n",
      "Train Epoch: 20 [320/2025 (16%)]\tLoss: 0.019181\n",
      "Train Epoch: 20 [480/2025 (24%)]\tLoss: 0.027541\n",
      "Train Epoch: 20 [640/2025 (31%)]\tLoss: 0.012613\n",
      "Train Epoch: 20 [800/2025 (39%)]\tLoss: 0.014461\n",
      "Train Epoch: 20 [960/2025 (47%)]\tLoss: 0.013125\n",
      "Train Epoch: 20 [1120/2025 (55%)]\tLoss: 0.039984\n",
      "Train Epoch: 20 [1280/2025 (63%)]\tLoss: 0.002784\n",
      "Train Epoch: 20 [1440/2025 (71%)]\tLoss: 0.136371\n",
      "Train Epoch: 20 [1600/2025 (79%)]\tLoss: 0.009312\n",
      "Train Epoch: 20 [1760/2025 (87%)]\tLoss: 0.056396\n",
      "Train Epoch: 20 [1920/2025 (94%)]\tLoss: 0.044979\n",
      "\n",
      "Test set: Average loss: 0.0522, Accuracy: 199/225 (88%)\n",
      "\n",
      "Train Epoch: 21 [0/2025 (0%)]\tLoss: 0.067833\n",
      "Train Epoch: 21 [160/2025 (8%)]\tLoss: 0.213475\n",
      "Train Epoch: 21 [320/2025 (16%)]\tLoss: 0.003977\n",
      "Train Epoch: 21 [480/2025 (24%)]\tLoss: 0.230503\n",
      "Train Epoch: 21 [640/2025 (31%)]\tLoss: 0.023265\n",
      "Train Epoch: 21 [800/2025 (39%)]\tLoss: 0.687708\n",
      "Train Epoch: 21 [960/2025 (47%)]\tLoss: 0.028306\n",
      "Train Epoch: 21 [1120/2025 (55%)]\tLoss: 0.169360\n",
      "Train Epoch: 21 [1280/2025 (63%)]\tLoss: 0.085352\n",
      "Train Epoch: 21 [1440/2025 (71%)]\tLoss: 0.070014\n",
      "Train Epoch: 21 [1600/2025 (79%)]\tLoss: 0.092713\n",
      "Train Epoch: 21 [1760/2025 (87%)]\tLoss: 0.006790\n",
      "Train Epoch: 21 [1920/2025 (94%)]\tLoss: 0.148375\n",
      "\n",
      "Test set: Average loss: 0.0533, Accuracy: 192/225 (85%)\n",
      "\n",
      "Train Epoch: 22 [0/2025 (0%)]\tLoss: 0.005549\n",
      "Train Epoch: 22 [160/2025 (8%)]\tLoss: 0.039825\n",
      "Train Epoch: 22 [320/2025 (16%)]\tLoss: 0.023054\n",
      "Train Epoch: 22 [480/2025 (24%)]\tLoss: 0.052241\n",
      "Train Epoch: 22 [640/2025 (31%)]\tLoss: 0.021622\n",
      "Train Epoch: 22 [800/2025 (39%)]\tLoss: 0.089479\n",
      "Train Epoch: 22 [960/2025 (47%)]\tLoss: 0.117365\n",
      "Train Epoch: 22 [1120/2025 (55%)]\tLoss: 0.010679\n",
      "Train Epoch: 22 [1280/2025 (63%)]\tLoss: 0.034695\n",
      "Train Epoch: 22 [1440/2025 (71%)]\tLoss: 0.082401\n",
      "Train Epoch: 22 [1600/2025 (79%)]\tLoss: 0.006974\n",
      "Train Epoch: 22 [1760/2025 (87%)]\tLoss: 0.147595\n",
      "Train Epoch: 22 [1920/2025 (94%)]\tLoss: 0.032902\n",
      "\n",
      "Test set: Average loss: 0.0456, Accuracy: 197/225 (88%)\n",
      "\n",
      "Train Epoch: 23 [0/2025 (0%)]\tLoss: 0.016740\n",
      "Train Epoch: 23 [160/2025 (8%)]\tLoss: 0.009550\n",
      "Train Epoch: 23 [320/2025 (16%)]\tLoss: 0.059462\n",
      "Train Epoch: 23 [480/2025 (24%)]\tLoss: 0.023941\n",
      "Train Epoch: 23 [640/2025 (31%)]\tLoss: 0.074830\n",
      "Train Epoch: 23 [800/2025 (39%)]\tLoss: 0.004797\n",
      "Train Epoch: 23 [960/2025 (47%)]\tLoss: 0.023905\n",
      "Train Epoch: 23 [1120/2025 (55%)]\tLoss: 0.004764\n",
      "Train Epoch: 23 [1280/2025 (63%)]\tLoss: 0.014414\n",
      "Train Epoch: 23 [1440/2025 (71%)]\tLoss: 0.011684\n",
      "Train Epoch: 23 [1600/2025 (79%)]\tLoss: 0.005566\n",
      "Train Epoch: 23 [1760/2025 (87%)]\tLoss: 0.003793\n",
      "Train Epoch: 23 [1920/2025 (94%)]\tLoss: 0.045234\n",
      "\n",
      "Test set: Average loss: 0.0326, Accuracy: 199/225 (88%)\n",
      "\n",
      "Train Epoch: 24 [0/2025 (0%)]\tLoss: 0.001871\n",
      "Train Epoch: 24 [160/2025 (8%)]\tLoss: 0.007511\n",
      "Train Epoch: 24 [320/2025 (16%)]\tLoss: 0.004979\n",
      "Train Epoch: 24 [480/2025 (24%)]\tLoss: 0.075640\n",
      "Train Epoch: 24 [640/2025 (31%)]\tLoss: 0.003939\n",
      "Train Epoch: 24 [800/2025 (39%)]\tLoss: 0.004209\n",
      "Train Epoch: 24 [960/2025 (47%)]\tLoss: 0.003206\n",
      "Train Epoch: 24 [1120/2025 (55%)]\tLoss: 0.017588\n",
      "Train Epoch: 24 [1280/2025 (63%)]\tLoss: 0.013835\n",
      "Train Epoch: 24 [1440/2025 (71%)]\tLoss: 0.066864\n",
      "Train Epoch: 24 [1600/2025 (79%)]\tLoss: 0.003277\n",
      "Train Epoch: 24 [1760/2025 (87%)]\tLoss: 0.003291\n",
      "Train Epoch: 24 [1920/2025 (94%)]\tLoss: 0.006336\n",
      "\n",
      "Test set: Average loss: 0.0738, Accuracy: 194/225 (86%)\n",
      "\n",
      "Train Epoch: 25 [0/2025 (0%)]\tLoss: 0.020675\n",
      "Train Epoch: 25 [160/2025 (8%)]\tLoss: 0.004194\n",
      "Train Epoch: 25 [320/2025 (16%)]\tLoss: 0.023256\n",
      "Train Epoch: 25 [480/2025 (24%)]\tLoss: 0.001177\n",
      "Train Epoch: 25 [640/2025 (31%)]\tLoss: 0.010434\n",
      "Train Epoch: 25 [800/2025 (39%)]\tLoss: 0.002432\n",
      "Train Epoch: 25 [960/2025 (47%)]\tLoss: 0.000703\n",
      "Train Epoch: 25 [1120/2025 (55%)]\tLoss: 0.000552\n",
      "Train Epoch: 25 [1280/2025 (63%)]\tLoss: 0.012755\n",
      "Train Epoch: 25 [1440/2025 (71%)]\tLoss: 0.030482\n",
      "Train Epoch: 25 [1600/2025 (79%)]\tLoss: 0.033872\n",
      "Train Epoch: 25 [1760/2025 (87%)]\tLoss: 0.002336\n",
      "Train Epoch: 25 [1920/2025 (94%)]\tLoss: 0.000758\n",
      "\n",
      "Test set: Average loss: 0.0575, Accuracy: 195/225 (87%)\n",
      "\n",
      "Train Epoch: 26 [0/2025 (0%)]\tLoss: 0.005260\n",
      "Train Epoch: 26 [160/2025 (8%)]\tLoss: 0.029168\n",
      "Train Epoch: 26 [320/2025 (16%)]\tLoss: 0.003344\n",
      "Train Epoch: 26 [480/2025 (24%)]\tLoss: 0.011486\n",
      "Train Epoch: 26 [640/2025 (31%)]\tLoss: 0.006384\n",
      "Train Epoch: 26 [800/2025 (39%)]\tLoss: 0.003337\n",
      "Train Epoch: 26 [960/2025 (47%)]\tLoss: 0.004775\n",
      "Train Epoch: 26 [1120/2025 (55%)]\tLoss: 0.003465\n",
      "Train Epoch: 26 [1280/2025 (63%)]\tLoss: 0.004626\n",
      "Train Epoch: 26 [1440/2025 (71%)]\tLoss: 0.297449\n",
      "Train Epoch: 26 [1600/2025 (79%)]\tLoss: 0.176328\n",
      "Train Epoch: 26 [1760/2025 (87%)]\tLoss: 0.035878\n",
      "Train Epoch: 26 [1920/2025 (94%)]\tLoss: 0.011700\n",
      "\n",
      "Test set: Average loss: 0.0706, Accuracy: 190/225 (84%)\n",
      "\n",
      "Train Epoch: 27 [0/2025 (0%)]\tLoss: 0.014740\n",
      "Train Epoch: 27 [160/2025 (8%)]\tLoss: 0.012352\n",
      "Train Epoch: 27 [320/2025 (16%)]\tLoss: 0.050470\n",
      "Train Epoch: 27 [480/2025 (24%)]\tLoss: 0.063185\n",
      "Train Epoch: 27 [640/2025 (31%)]\tLoss: 0.054568\n",
      "Train Epoch: 27 [800/2025 (39%)]\tLoss: 0.097089\n",
      "Train Epoch: 27 [960/2025 (47%)]\tLoss: 0.070491\n",
      "Train Epoch: 27 [1120/2025 (55%)]\tLoss: 0.020225\n",
      "Train Epoch: 27 [1280/2025 (63%)]\tLoss: 0.013441\n",
      "Train Epoch: 27 [1440/2025 (71%)]\tLoss: 0.068009\n",
      "Train Epoch: 27 [1600/2025 (79%)]\tLoss: 0.107499\n",
      "Train Epoch: 27 [1760/2025 (87%)]\tLoss: 0.075135\n",
      "Train Epoch: 27 [1920/2025 (94%)]\tLoss: 0.001671\n",
      "\n",
      "Test set: Average loss: 0.0679, Accuracy: 190/225 (84%)\n",
      "\n",
      "Train Epoch: 28 [0/2025 (0%)]\tLoss: 0.018632\n",
      "Train Epoch: 28 [160/2025 (8%)]\tLoss: 0.116170\n",
      "Train Epoch: 28 [320/2025 (16%)]\tLoss: 0.023066\n",
      "Train Epoch: 28 [480/2025 (24%)]\tLoss: 0.021167\n",
      "Train Epoch: 28 [640/2025 (31%)]\tLoss: 0.012150\n",
      "Train Epoch: 28 [800/2025 (39%)]\tLoss: 0.032751\n",
      "Train Epoch: 28 [960/2025 (47%)]\tLoss: 0.026298\n",
      "Train Epoch: 28 [1120/2025 (55%)]\tLoss: 0.046253\n",
      "Train Epoch: 28 [1280/2025 (63%)]\tLoss: 0.001084\n",
      "Train Epoch: 28 [1440/2025 (71%)]\tLoss: 0.001479\n",
      "Train Epoch: 28 [1600/2025 (79%)]\tLoss: 0.362032\n",
      "Train Epoch: 28 [1760/2025 (87%)]\tLoss: 0.023307\n",
      "Train Epoch: 28 [1920/2025 (94%)]\tLoss: 0.023331\n",
      "\n",
      "Test set: Average loss: 0.0650, Accuracy: 184/225 (82%)\n",
      "\n",
      "Train Epoch: 29 [0/2025 (0%)]\tLoss: 0.029735\n",
      "Train Epoch: 29 [160/2025 (8%)]\tLoss: 0.598090\n",
      "Train Epoch: 29 [320/2025 (16%)]\tLoss: 0.092147\n",
      "Train Epoch: 29 [480/2025 (24%)]\tLoss: 0.008879\n",
      "Train Epoch: 29 [640/2025 (31%)]\tLoss: 0.056759\n",
      "Train Epoch: 29 [800/2025 (39%)]\tLoss: 0.075403\n",
      "Train Epoch: 29 [960/2025 (47%)]\tLoss: 0.071959\n",
      "Train Epoch: 29 [1120/2025 (55%)]\tLoss: 0.001272\n",
      "Train Epoch: 29 [1280/2025 (63%)]\tLoss: 0.079931\n",
      "Train Epoch: 29 [1440/2025 (71%)]\tLoss: 0.022645\n",
      "Train Epoch: 29 [1600/2025 (79%)]\tLoss: 0.008322\n",
      "Train Epoch: 29 [1760/2025 (87%)]\tLoss: 0.015037\n",
      "Train Epoch: 29 [1920/2025 (94%)]\tLoss: 0.012994\n",
      "\n",
      "Test set: Average loss: 0.0343, Accuracy: 197/225 (88%)\n",
      "\n",
      "Train Epoch: 30 [0/2025 (0%)]\tLoss: 0.021074\n",
      "Train Epoch: 30 [160/2025 (8%)]\tLoss: 0.009799\n",
      "Train Epoch: 30 [320/2025 (16%)]\tLoss: 0.006308\n",
      "Train Epoch: 30 [480/2025 (24%)]\tLoss: 0.005484\n",
      "Train Epoch: 30 [640/2025 (31%)]\tLoss: 0.062139\n",
      "Train Epoch: 30 [800/2025 (39%)]\tLoss: 0.003170\n",
      "Train Epoch: 30 [960/2025 (47%)]\tLoss: 0.001810\n",
      "Train Epoch: 30 [1120/2025 (55%)]\tLoss: 0.005358\n",
      "Train Epoch: 30 [1280/2025 (63%)]\tLoss: 0.002599\n",
      "Train Epoch: 30 [1440/2025 (71%)]\tLoss: 0.006874\n",
      "Train Epoch: 30 [1600/2025 (79%)]\tLoss: 0.085742\n",
      "Train Epoch: 30 [1760/2025 (87%)]\tLoss: 0.013476\n",
      "Train Epoch: 30 [1920/2025 (94%)]\tLoss: 0.004993\n",
      "\n",
      "Test set: Average loss: 0.0346, Accuracy: 197/225 (88%)\n",
      "\n",
      "Train Epoch: 31 [0/2025 (0%)]\tLoss: 0.001992\n",
      "Train Epoch: 31 [160/2025 (8%)]\tLoss: 0.015288\n",
      "Train Epoch: 31 [320/2025 (16%)]\tLoss: 0.003434\n",
      "Train Epoch: 31 [480/2025 (24%)]\tLoss: 0.000702\n",
      "Train Epoch: 31 [640/2025 (31%)]\tLoss: 0.000327\n",
      "Train Epoch: 31 [800/2025 (39%)]\tLoss: 0.001094\n",
      "Train Epoch: 31 [960/2025 (47%)]\tLoss: 0.003419\n",
      "Train Epoch: 31 [1120/2025 (55%)]\tLoss: 0.010635\n",
      "Train Epoch: 31 [1280/2025 (63%)]\tLoss: 0.002022\n",
      "Train Epoch: 31 [1440/2025 (71%)]\tLoss: 0.002505\n",
      "Train Epoch: 31 [1600/2025 (79%)]\tLoss: 0.000925\n",
      "Train Epoch: 31 [1760/2025 (87%)]\tLoss: 0.006185\n",
      "Train Epoch: 31 [1920/2025 (94%)]\tLoss: 0.002549\n",
      "\n",
      "Test set: Average loss: 0.0387, Accuracy: 198/225 (88%)\n",
      "\n",
      "Train Epoch: 32 [0/2025 (0%)]\tLoss: 0.001360\n",
      "Train Epoch: 32 [160/2025 (8%)]\tLoss: 0.002070\n",
      "Train Epoch: 32 [320/2025 (16%)]\tLoss: 0.002814\n",
      "Train Epoch: 32 [480/2025 (24%)]\tLoss: 0.000579\n",
      "Train Epoch: 32 [640/2025 (31%)]\tLoss: 0.002388\n",
      "Train Epoch: 32 [800/2025 (39%)]\tLoss: 0.006731\n",
      "Train Epoch: 32 [960/2025 (47%)]\tLoss: 0.000266\n",
      "Train Epoch: 32 [1120/2025 (55%)]\tLoss: 0.002221\n",
      "Train Epoch: 32 [1280/2025 (63%)]\tLoss: 0.001212\n",
      "Train Epoch: 32 [1440/2025 (71%)]\tLoss: 0.001804\n",
      "Train Epoch: 32 [1600/2025 (79%)]\tLoss: 0.003650\n",
      "Train Epoch: 32 [1760/2025 (87%)]\tLoss: 0.005531\n",
      "Train Epoch: 32 [1920/2025 (94%)]\tLoss: 0.000522\n",
      "\n",
      "Test set: Average loss: 0.0426, Accuracy: 198/225 (88%)\n",
      "\n",
      "Train Epoch: 33 [0/2025 (0%)]\tLoss: 0.001966\n",
      "Train Epoch: 33 [160/2025 (8%)]\tLoss: 0.002943\n",
      "Train Epoch: 33 [320/2025 (16%)]\tLoss: 0.001727\n",
      "Train Epoch: 33 [480/2025 (24%)]\tLoss: 0.000785\n",
      "Train Epoch: 33 [640/2025 (31%)]\tLoss: 0.001591\n",
      "Train Epoch: 33 [800/2025 (39%)]\tLoss: 0.000190\n",
      "Train Epoch: 33 [960/2025 (47%)]\tLoss: 0.002487\n",
      "Train Epoch: 33 [1120/2025 (55%)]\tLoss: 0.001423\n",
      "Train Epoch: 33 [1280/2025 (63%)]\tLoss: 0.003499\n",
      "Train Epoch: 33 [1440/2025 (71%)]\tLoss: 0.004382\n",
      "Train Epoch: 33 [1600/2025 (79%)]\tLoss: 0.000526\n",
      "Train Epoch: 33 [1760/2025 (87%)]\tLoss: 0.001269\n",
      "Train Epoch: 33 [1920/2025 (94%)]\tLoss: 0.000644\n",
      "\n",
      "Test set: Average loss: 0.0421, Accuracy: 199/225 (88%)\n",
      "\n",
      "Train Epoch: 34 [0/2025 (0%)]\tLoss: 0.000476\n",
      "Train Epoch: 34 [160/2025 (8%)]\tLoss: 0.000541\n",
      "Train Epoch: 34 [320/2025 (16%)]\tLoss: 0.000361\n",
      "Train Epoch: 34 [480/2025 (24%)]\tLoss: 0.000555\n",
      "Train Epoch: 34 [640/2025 (31%)]\tLoss: 0.000348\n",
      "Train Epoch: 34 [800/2025 (39%)]\tLoss: 0.000665\n",
      "Train Epoch: 34 [960/2025 (47%)]\tLoss: 0.001298\n",
      "Train Epoch: 34 [1120/2025 (55%)]\tLoss: 0.001907\n",
      "Train Epoch: 34 [1280/2025 (63%)]\tLoss: 0.003557\n",
      "Train Epoch: 34 [1440/2025 (71%)]\tLoss: 0.000228\n",
      "Train Epoch: 34 [1600/2025 (79%)]\tLoss: 0.001501\n",
      "Train Epoch: 34 [1760/2025 (87%)]\tLoss: 0.000717\n",
      "Train Epoch: 34 [1920/2025 (94%)]\tLoss: 0.000968\n",
      "\n",
      "Test set: Average loss: 0.0439, Accuracy: 199/225 (88%)\n",
      "\n",
      "Train Epoch: 35 [0/2025 (0%)]\tLoss: 0.000715\n",
      "Train Epoch: 35 [160/2025 (8%)]\tLoss: 0.000940\n",
      "Train Epoch: 35 [320/2025 (16%)]\tLoss: 0.000658\n",
      "Train Epoch: 35 [480/2025 (24%)]\tLoss: 0.001123\n",
      "Train Epoch: 35 [640/2025 (31%)]\tLoss: 0.001218\n",
      "Train Epoch: 35 [800/2025 (39%)]\tLoss: 0.003429\n",
      "Train Epoch: 35 [960/2025 (47%)]\tLoss: 0.000991\n",
      "Train Epoch: 35 [1120/2025 (55%)]\tLoss: 0.000380\n",
      "Train Epoch: 35 [1280/2025 (63%)]\tLoss: 0.003362\n",
      "Train Epoch: 35 [1440/2025 (71%)]\tLoss: 0.002259\n",
      "Train Epoch: 35 [1600/2025 (79%)]\tLoss: 0.000147\n",
      "Train Epoch: 35 [1760/2025 (87%)]\tLoss: 0.001071\n",
      "Train Epoch: 35 [1920/2025 (94%)]\tLoss: 0.000348\n",
      "\n",
      "Test set: Average loss: 0.0454, Accuracy: 199/225 (88%)\n",
      "\n",
      "Train Epoch: 36 [0/2025 (0%)]\tLoss: 0.001799\n",
      "Train Epoch: 36 [160/2025 (8%)]\tLoss: 0.000416\n",
      "Train Epoch: 36 [320/2025 (16%)]\tLoss: 0.001389\n",
      "Train Epoch: 36 [480/2025 (24%)]\tLoss: 0.000956\n",
      "Train Epoch: 36 [640/2025 (31%)]\tLoss: 0.000319\n",
      "Train Epoch: 36 [800/2025 (39%)]\tLoss: 0.000046\n",
      "Train Epoch: 36 [960/2025 (47%)]\tLoss: 0.000182\n",
      "Train Epoch: 36 [1120/2025 (55%)]\tLoss: 0.001177\n",
      "Train Epoch: 36 [1280/2025 (63%)]\tLoss: 0.000256\n",
      "Train Epoch: 36 [1440/2025 (71%)]\tLoss: 0.000386\n",
      "Train Epoch: 36 [1600/2025 (79%)]\tLoss: 0.001038\n",
      "Train Epoch: 36 [1760/2025 (87%)]\tLoss: 0.000352\n",
      "Train Epoch: 36 [1920/2025 (94%)]\tLoss: 0.001396\n",
      "\n",
      "Test set: Average loss: 0.0468, Accuracy: 199/225 (88%)\n",
      "\n",
      "Train Epoch: 37 [0/2025 (0%)]\tLoss: 0.001052\n",
      "Train Epoch: 37 [160/2025 (8%)]\tLoss: 0.000324\n",
      "Train Epoch: 37 [320/2025 (16%)]\tLoss: 0.000203\n",
      "Train Epoch: 37 [480/2025 (24%)]\tLoss: 0.000612\n",
      "Train Epoch: 37 [640/2025 (31%)]\tLoss: 0.001091\n",
      "Train Epoch: 37 [800/2025 (39%)]\tLoss: 0.000261\n",
      "Train Epoch: 37 [960/2025 (47%)]\tLoss: 0.000235\n",
      "Train Epoch: 37 [1120/2025 (55%)]\tLoss: 0.000519\n",
      "Train Epoch: 37 [1280/2025 (63%)]\tLoss: 0.000169\n",
      "Train Epoch: 37 [1440/2025 (71%)]\tLoss: 0.000720\n",
      "Train Epoch: 37 [1600/2025 (79%)]\tLoss: 0.000245\n",
      "Train Epoch: 37 [1760/2025 (87%)]\tLoss: 0.000537\n",
      "Train Epoch: 37 [1920/2025 (94%)]\tLoss: 0.001603\n",
      "\n",
      "Test set: Average loss: 0.0470, Accuracy: 198/225 (88%)\n",
      "\n",
      "Train Epoch: 38 [0/2025 (0%)]\tLoss: 0.000112\n",
      "Train Epoch: 38 [160/2025 (8%)]\tLoss: 0.000354\n",
      "Train Epoch: 38 [320/2025 (16%)]\tLoss: 0.000366\n",
      "Train Epoch: 38 [480/2025 (24%)]\tLoss: 0.000087\n",
      "Train Epoch: 38 [640/2025 (31%)]\tLoss: 0.000274\n",
      "Train Epoch: 38 [800/2025 (39%)]\tLoss: 0.000458\n",
      "Train Epoch: 38 [960/2025 (47%)]\tLoss: 0.000214\n",
      "Train Epoch: 38 [1120/2025 (55%)]\tLoss: 0.000512\n",
      "Train Epoch: 38 [1280/2025 (63%)]\tLoss: 0.000523\n",
      "Train Epoch: 38 [1440/2025 (71%)]\tLoss: 0.000297\n",
      "Train Epoch: 38 [1600/2025 (79%)]\tLoss: 0.000548\n",
      "Train Epoch: 38 [1760/2025 (87%)]\tLoss: 0.000424\n",
      "Train Epoch: 38 [1920/2025 (94%)]\tLoss: 0.000544\n",
      "\n",
      "Test set: Average loss: 0.0489, Accuracy: 199/225 (88%)\n",
      "\n",
      "Train Epoch: 39 [0/2025 (0%)]\tLoss: 0.001857\n",
      "Train Epoch: 39 [160/2025 (8%)]\tLoss: 0.000184\n",
      "Train Epoch: 39 [320/2025 (16%)]\tLoss: 0.000460\n",
      "Train Epoch: 39 [480/2025 (24%)]\tLoss: 0.000970\n",
      "Train Epoch: 39 [640/2025 (31%)]\tLoss: 0.000307\n",
      "Train Epoch: 39 [800/2025 (39%)]\tLoss: 0.000350\n",
      "Train Epoch: 39 [960/2025 (47%)]\tLoss: 0.000239\n",
      "Train Epoch: 39 [1120/2025 (55%)]\tLoss: 0.001553\n",
      "Train Epoch: 39 [1280/2025 (63%)]\tLoss: 0.000133\n",
      "Train Epoch: 39 [1440/2025 (71%)]\tLoss: 0.000150\n",
      "Train Epoch: 39 [1600/2025 (79%)]\tLoss: 0.000256\n",
      "Train Epoch: 39 [1760/2025 (87%)]\tLoss: 0.000313\n",
      "Train Epoch: 39 [1920/2025 (94%)]\tLoss: 0.000569\n",
      "\n",
      "Test set: Average loss: 0.0501, Accuracy: 199/225 (88%)\n",
      "\n",
      "Train Epoch: 40 [0/2025 (0%)]\tLoss: 0.002391\n",
      "Train Epoch: 40 [160/2025 (8%)]\tLoss: 0.000052\n",
      "Train Epoch: 40 [320/2025 (16%)]\tLoss: 0.000306\n",
      "Train Epoch: 40 [480/2025 (24%)]\tLoss: 0.000627\n",
      "Train Epoch: 40 [640/2025 (31%)]\tLoss: 0.000233\n",
      "Train Epoch: 40 [800/2025 (39%)]\tLoss: 0.000252\n",
      "Train Epoch: 40 [960/2025 (47%)]\tLoss: 0.000575\n",
      "Train Epoch: 40 [1120/2025 (55%)]\tLoss: 0.000508\n",
      "Train Epoch: 40 [1280/2025 (63%)]\tLoss: 0.000042\n",
      "Train Epoch: 40 [1440/2025 (71%)]\tLoss: 0.000110\n",
      "Train Epoch: 40 [1600/2025 (79%)]\tLoss: 0.000255\n",
      "Train Epoch: 40 [1760/2025 (87%)]\tLoss: 0.000223\n",
      "Train Epoch: 40 [1920/2025 (94%)]\tLoss: 0.000334\n",
      "\n",
      "Test set: Average loss: 0.0501, Accuracy: 198/225 (88%)\n",
      "\n",
      "Train Epoch: 41 [0/2025 (0%)]\tLoss: 0.000446\n",
      "Train Epoch: 41 [160/2025 (8%)]\tLoss: 0.001032\n",
      "Train Epoch: 41 [320/2025 (16%)]\tLoss: 0.000308\n",
      "Train Epoch: 41 [480/2025 (24%)]\tLoss: 0.000045\n",
      "Train Epoch: 41 [640/2025 (31%)]\tLoss: 0.000274\n",
      "Train Epoch: 41 [800/2025 (39%)]\tLoss: 0.000370\n",
      "Train Epoch: 41 [960/2025 (47%)]\tLoss: 0.000077\n",
      "Train Epoch: 41 [1120/2025 (55%)]\tLoss: 0.000366\n",
      "Train Epoch: 41 [1280/2025 (63%)]\tLoss: 0.000641\n",
      "Train Epoch: 41 [1440/2025 (71%)]\tLoss: 0.000022\n",
      "Train Epoch: 41 [1600/2025 (79%)]\tLoss: 0.000307\n",
      "Train Epoch: 41 [1760/2025 (87%)]\tLoss: 0.000341\n",
      "Train Epoch: 41 [1920/2025 (94%)]\tLoss: 0.000227\n",
      "\n",
      "Test set: Average loss: 0.0512, Accuracy: 198/225 (88%)\n",
      "\n",
      "Train Epoch: 42 [0/2025 (0%)]\tLoss: 0.000035\n",
      "Train Epoch: 42 [160/2025 (8%)]\tLoss: 0.000216\n",
      "Train Epoch: 42 [320/2025 (16%)]\tLoss: 0.000082\n",
      "Train Epoch: 42 [480/2025 (24%)]\tLoss: 0.001023\n",
      "Train Epoch: 42 [640/2025 (31%)]\tLoss: 0.000075\n",
      "Train Epoch: 42 [800/2025 (39%)]\tLoss: 0.000234\n",
      "Train Epoch: 42 [960/2025 (47%)]\tLoss: 0.000220\n",
      "Train Epoch: 42 [1120/2025 (55%)]\tLoss: 0.000263\n",
      "Train Epoch: 42 [1280/2025 (63%)]\tLoss: 0.000312\n",
      "Train Epoch: 42 [1440/2025 (71%)]\tLoss: 0.000135\n",
      "Train Epoch: 42 [1600/2025 (79%)]\tLoss: 0.001374\n",
      "Train Epoch: 42 [1760/2025 (87%)]\tLoss: 0.000403\n",
      "Train Epoch: 42 [1920/2025 (94%)]\tLoss: 0.000147\n",
      "\n",
      "Test set: Average loss: 0.0517, Accuracy: 197/225 (88%)\n",
      "\n",
      "Train Epoch: 43 [0/2025 (0%)]\tLoss: 0.000322\n",
      "Train Epoch: 43 [160/2025 (8%)]\tLoss: 0.000052\n",
      "Train Epoch: 43 [320/2025 (16%)]\tLoss: 0.000265\n",
      "Train Epoch: 43 [480/2025 (24%)]\tLoss: 0.000244\n",
      "Train Epoch: 43 [640/2025 (31%)]\tLoss: 0.000277\n",
      "Train Epoch: 43 [800/2025 (39%)]\tLoss: 0.000200\n",
      "Train Epoch: 43 [960/2025 (47%)]\tLoss: 0.000330\n",
      "Train Epoch: 43 [1120/2025 (55%)]\tLoss: 0.000161\n",
      "Train Epoch: 43 [1280/2025 (63%)]\tLoss: 0.000191\n",
      "Train Epoch: 43 [1440/2025 (71%)]\tLoss: 0.000412\n",
      "Train Epoch: 43 [1600/2025 (79%)]\tLoss: 0.000088\n",
      "Train Epoch: 43 [1760/2025 (87%)]\tLoss: 0.000269\n",
      "Train Epoch: 43 [1920/2025 (94%)]\tLoss: 0.000092\n",
      "\n",
      "Test set: Average loss: 0.0527, Accuracy: 197/225 (88%)\n",
      "\n",
      "Train Epoch: 44 [0/2025 (0%)]\tLoss: 0.000447\n",
      "Train Epoch: 44 [160/2025 (8%)]\tLoss: 0.000040\n",
      "Train Epoch: 44 [320/2025 (16%)]\tLoss: 0.000088\n",
      "Train Epoch: 44 [480/2025 (24%)]\tLoss: 0.000206\n",
      "Train Epoch: 44 [640/2025 (31%)]\tLoss: 0.000134\n",
      "Train Epoch: 44 [800/2025 (39%)]\tLoss: 0.000045\n",
      "Train Epoch: 44 [960/2025 (47%)]\tLoss: 0.000122\n",
      "Train Epoch: 44 [1120/2025 (55%)]\tLoss: 0.000227\n",
      "Train Epoch: 44 [1280/2025 (63%)]\tLoss: 0.000290\n",
      "Train Epoch: 44 [1440/2025 (71%)]\tLoss: 0.000089\n",
      "Train Epoch: 44 [1600/2025 (79%)]\tLoss: 0.000272\n",
      "Train Epoch: 44 [1760/2025 (87%)]\tLoss: 0.000188\n",
      "Train Epoch: 44 [1920/2025 (94%)]\tLoss: 0.000359\n",
      "\n",
      "Test set: Average loss: 0.0530, Accuracy: 197/225 (88%)\n",
      "\n",
      "Train Epoch: 45 [0/2025 (0%)]\tLoss: 0.000275\n",
      "Train Epoch: 45 [160/2025 (8%)]\tLoss: 0.000202\n",
      "Train Epoch: 45 [320/2025 (16%)]\tLoss: 0.000009\n",
      "Train Epoch: 45 [480/2025 (24%)]\tLoss: 0.000123\n",
      "Train Epoch: 45 [640/2025 (31%)]\tLoss: 0.000034\n",
      "Train Epoch: 45 [800/2025 (39%)]\tLoss: 0.000241\n",
      "Train Epoch: 45 [960/2025 (47%)]\tLoss: 0.000167\n",
      "Train Epoch: 45 [1120/2025 (55%)]\tLoss: 0.000308\n",
      "Train Epoch: 45 [1280/2025 (63%)]\tLoss: 0.000237\n",
      "Train Epoch: 45 [1440/2025 (71%)]\tLoss: 0.000310\n",
      "Train Epoch: 45 [1600/2025 (79%)]\tLoss: 0.000117\n",
      "Train Epoch: 45 [1760/2025 (87%)]\tLoss: 0.000015\n",
      "Train Epoch: 45 [1920/2025 (94%)]\tLoss: 0.000283\n",
      "\n",
      "Test set: Average loss: 0.0541, Accuracy: 197/225 (88%)\n",
      "\n",
      "Train Epoch: 46 [0/2025 (0%)]\tLoss: 0.000095\n",
      "Train Epoch: 46 [160/2025 (8%)]\tLoss: 0.000143\n",
      "Train Epoch: 46 [320/2025 (16%)]\tLoss: 0.000033\n",
      "Train Epoch: 46 [480/2025 (24%)]\tLoss: 0.000013\n",
      "Train Epoch: 46 [640/2025 (31%)]\tLoss: 0.000027\n",
      "Train Epoch: 46 [800/2025 (39%)]\tLoss: 0.000108\n",
      "Train Epoch: 46 [960/2025 (47%)]\tLoss: 0.000149\n",
      "Train Epoch: 46 [1120/2025 (55%)]\tLoss: 0.000125\n",
      "Train Epoch: 46 [1280/2025 (63%)]\tLoss: 0.000109\n",
      "Train Epoch: 46 [1440/2025 (71%)]\tLoss: 0.000017\n",
      "Train Epoch: 46 [1600/2025 (79%)]\tLoss: 0.000042\n",
      "Train Epoch: 46 [1760/2025 (87%)]\tLoss: 0.000024\n",
      "Train Epoch: 46 [1920/2025 (94%)]\tLoss: 0.000104\n",
      "\n",
      "Test set: Average loss: 0.0551, Accuracy: 199/225 (88%)\n",
      "\n",
      "Train Epoch: 47 [0/2025 (0%)]\tLoss: 0.000182\n",
      "Train Epoch: 47 [160/2025 (8%)]\tLoss: 0.000089\n",
      "Train Epoch: 47 [320/2025 (16%)]\tLoss: 0.000067\n",
      "Train Epoch: 47 [480/2025 (24%)]\tLoss: 0.000037\n",
      "Train Epoch: 47 [640/2025 (31%)]\tLoss: 0.000062\n",
      "Train Epoch: 47 [800/2025 (39%)]\tLoss: 0.000197\n",
      "Train Epoch: 47 [960/2025 (47%)]\tLoss: 0.000365\n",
      "Train Epoch: 47 [1120/2025 (55%)]\tLoss: 0.000235\n",
      "Train Epoch: 47 [1280/2025 (63%)]\tLoss: 0.000262\n",
      "Train Epoch: 47 [1440/2025 (71%)]\tLoss: 0.000334\n",
      "Train Epoch: 47 [1600/2025 (79%)]\tLoss: 0.000188\n",
      "Train Epoch: 47 [1760/2025 (87%)]\tLoss: 0.000132\n",
      "Train Epoch: 47 [1920/2025 (94%)]\tLoss: 0.000184\n",
      "\n",
      "Test set: Average loss: 0.0551, Accuracy: 197/225 (88%)\n",
      "\n",
      "Train Epoch: 48 [0/2025 (0%)]\tLoss: 0.000107\n",
      "Train Epoch: 48 [160/2025 (8%)]\tLoss: 0.000842\n",
      "Train Epoch: 48 [320/2025 (16%)]\tLoss: 0.000263\n",
      "Train Epoch: 48 [480/2025 (24%)]\tLoss: 0.000022\n",
      "Train Epoch: 48 [640/2025 (31%)]\tLoss: 0.000214\n",
      "Train Epoch: 48 [800/2025 (39%)]\tLoss: 0.000280\n",
      "Train Epoch: 48 [960/2025 (47%)]\tLoss: 0.000094\n",
      "Train Epoch: 48 [1120/2025 (55%)]\tLoss: 0.000048\n",
      "Train Epoch: 48 [1280/2025 (63%)]\tLoss: 0.000095\n",
      "Train Epoch: 48 [1440/2025 (71%)]\tLoss: 0.000045\n",
      "Train Epoch: 48 [1600/2025 (79%)]\tLoss: 0.000619\n",
      "Train Epoch: 48 [1760/2025 (87%)]\tLoss: 0.000567\n",
      "Train Epoch: 48 [1920/2025 (94%)]\tLoss: 0.000374\n",
      "\n",
      "Test set: Average loss: 0.0560, Accuracy: 198/225 (88%)\n",
      "\n",
      "Train Epoch: 49 [0/2025 (0%)]\tLoss: 0.000212\n",
      "Train Epoch: 49 [160/2025 (8%)]\tLoss: 0.000111\n",
      "Train Epoch: 49 [320/2025 (16%)]\tLoss: 0.000150\n",
      "Train Epoch: 49 [480/2025 (24%)]\tLoss: 0.000075\n",
      "Train Epoch: 49 [640/2025 (31%)]\tLoss: 0.000191\n",
      "Train Epoch: 49 [800/2025 (39%)]\tLoss: 0.000193\n",
      "Train Epoch: 49 [960/2025 (47%)]\tLoss: 0.000041\n",
      "Train Epoch: 49 [1120/2025 (55%)]\tLoss: 0.000344\n",
      "Train Epoch: 49 [1280/2025 (63%)]\tLoss: 0.000151\n",
      "Train Epoch: 49 [1440/2025 (71%)]\tLoss: 0.000047\n",
      "Train Epoch: 49 [1600/2025 (79%)]\tLoss: 0.000050\n",
      "Train Epoch: 49 [1760/2025 (87%)]\tLoss: 0.000062\n",
      "Train Epoch: 49 [1920/2025 (94%)]\tLoss: 0.000090\n",
      "\n",
      "Test set: Average loss: 0.0565, Accuracy: 198/225 (88%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import KFold\n",
    "batch_size =16\n",
    "\n",
    "#kf = KFold(n_splits=10)\n",
    "\n",
    "#for train_size,test_size in kf.split(dataset):\n",
    "#  result = train_size, test_size\n",
    "#  train_data = dataset.iloc[result[0]]\n",
    "#  test_data = dataset.iloc[result[1]]\n",
    "\n",
    "train_size = int(len(dataset) * 0.9)\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "\n",
    "dataset = MyDataset()\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset,[train_size,test_size])\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False)\n",
    "  \n",
    "model = Model()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)\n",
    "\n",
    "for epoch in range(1, 50):\n",
    "  train(epoch)\n",
    "  test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eG8DKg6Y9CJF"
   },
   "source": [
    "feature  Step 1: \n",
    "\n",
    "        'BrainSegNotVent', 'BrainSegNotVentSurf','VentricleChoroidVol',\n",
    "            'lhCortex', 'rhCortex', 'Cortex','lhCerebralWhiteMatter',\n",
    "       'rhCerebralWhiteMatter', 'CerebralWhiteMatter', 'SubCortGray',\n",
    "       'TotalGray','SupraTentorialNotVent',\n",
    "       'SupraTentorialNotVentVox', 'Mask', 'BrainSegVol-to-eTIV','lhSurfaceHoles',\n",
    "       'SurfaceHoles','EstimatedTotalIntraCranialVol','Left-Lateral-Ventricle',\n",
    "       'Left-Inf-Lat-Vent','Left-Cerebellum-Cortex','Left-Caudate',\n",
    "       'Left-Putamen','3rd-Ventricle','Left-Hippocampus', 'Left-Amygdala', 'CSF',\n",
    "       'Left-Accumbens-area','Left-choroid-plexus','Right-Lateral-Ventricle', 'Right-Inf-Lat-Vent',\n",
    "       'Right-Cerebellum-Cortex','Right-Caudate', 'Right-Putamen','Right-Hippocampus', 'Right-Amygdala',\n",
    "       'Right-Accumbens-area','Right-vessel','Right-choroid-plexus','WM-hypointensities',\n",
    "       'non-WM-hypointensities','Optic-Chiasm','CC_Central','CC_Mid_Anterior'\n",
    "\n",
    "Epoch 100 92%\n",
    "\n",
    "feature Step 2:\n",
    "\n",
    "       'VentricleChoroidVol',\n",
    "            'lhCortex', 'rhCortex', 'Cortex', 'SubCortGray',\n",
    "       'TotalGray','BrainSegVol-to-eTIV','Left-Lateral-Ventricle',\n",
    "       'Left-Inf-Lat-Vent',\n",
    "       'Left-Putamen','3rd-Ventricle','Left-Hippocampus', 'Left-Amygdala', 'CSF',\n",
    "       'Left-Accumbens-area','Left-choroid-plexus','Right-Lateral-Ventricle', 'Right-Inf-Lat-Vent'\n",
    "       ,'Right-Putamen','Right-Hippocampus', 'Right-Amygdala',\n",
    "       'Right-Accumbens-area','WM-hypointensities',\n",
    "       'non-WM-hypointensities','CC_Mid_Anterior'\n",
    "\n",
    "Epoch 100 90%"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "handheld.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
